services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile
      target: runtime
    command: ["uv", "run", "uvicorn", "autovisionai.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
    ports:
      - "8000:8000"
    volumes:
      - ./mlruns:/app/mlruns
      - ./mlartifacts:/app/mlartifacts
      - ./experiments:/app/experiments
    env_file:
      - .env
    environment:
      - ENV_MODE="prod"
      - WANDB_API_KEY=${WANDB_API_KEY}
      - WANDB_ENTITY=${WANDB_ENTITY}
      - PYTHONUNBUFFERED=1
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  ui:
    build:
      context: .
      dockerfile: docker/Dockerfile
      target: runtime
    command: ["uv", "run", "streamlit", "run", "src/autovisionai/ui/app.py", "--server.port", "8501", "--server.address", "0.0.0.0"]
    ports:
      - "8501:8501"
    volumes:
      - ./mlruns:/app/mlruns
      - ./mlartifacts:/app/mlartifacts
      - ./experiments:/app/experiments
    env_file:
      - .env
    environment:
      - ENV_MODE=${ENV_MODE}
      - WANDB_API_KEY=${WANDB_API_KEY}
      - WANDB_ENTITY=${WANDB_ENTITY}
      - PYTHONUNBUFFERED=1
    depends_on:
      api:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mlflow:
    build:
      context: .
      dockerfile: docker/Dockerfile.mlflow
    ports:
      - "8080:8080"
    volumes:
      - ./mlruns:/app/mlruns
      - ./mlartifacts:/app/mlartifacts
    environment:
      - PYTHONUNBUFFERED=1

  tensorboard:
    build:
      context: .
      dockerfile: docker/Dockerfile.tensorboard
    ports:
      - "6006:6006"
    volumes:
      - ./experiments:/app/experiments
    environment:
      - PYTHONUNBUFFERED=1
